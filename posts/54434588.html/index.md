# 通过split命令分割大文件

## 场景

线上出了问题，我需要去查找log来定位问题，但是由于线上数据量庞大，这些log文件每过一个小时就会自动回滚一次，尽管如此，有的log文件依然达到了五六g以上的大小。

对于这种巨大的log文件，常用的一些文本编辑器诸如EditPlus、Notepad++就不用说了，打开几百m的文件都会很卡，上g的直接程序崩溃。虽然UltraEdit对于大文件的读取会友好一些，但打开这种五六g的文件时也会陷入长时间的无响应状态。

后来我又得知了一个看log神器——glogg，打开五六g的大文件速度很快，但是有个问题，就是只能读取文件，不能编辑文件。毕竟我不只是要查看log，有时候还要对这些有用的log信息进行编辑。最后还是决定先把大文件分割成数个小文件，再用UltraEdit来查看这些文件。
<!--more-->

## 使用split命令分割大文件

在Linux下，切割和合并文件可以使用split和cat命令来实现。 
在Windows下，安装Git Bash也可以使用split和cat命令。

分割文件的命令是`split`，通过输入`split --help`可以查询帮助信息。假设现在有个6GB大小的文件`test.log`，这里简单介绍下几种分割的方式：

### 按大小分割文件

```cmd
split -b 1000000000 test.log
```

`-b`参数表示按字节大小进行分割，在数字后边要指定被分割的文件名。这里在输入文件名时有个小技巧，可以直接把该文件拖动到cmd窗口中，会自动输入该文件的具体目录。这里的文件还可以使用通配符，比如`split -b 1000000000 *`。

这个命令表示按1000000000byte的大小进行分割，近似于1GB，大概是953MB的大小。对于这个6GB大小的文件`test.log`，会被分割成6个小文件。这些小文件的命名是有规律的：xaa、xab、xac、xad、xae、xaf。如果你分割了非常多的小文件，当文件名到了`xyz`之后，会变成xzaaa、xzaab、xzaac、xzaad……所以不用担心小文件过多而导致文件重名什么的。

当然，上边的这种写法不够人性化，我们可以使用其他的单位来指定分割的大小：k、m。k表示KB，m表示MB。

`split -b 100k test.log`表示将test.log按照100KB的大小进行分割。
`split -b 100m test.log`表示将test.log按照100MB的大小进行分割。

### 按照所有行数加起来的最大字节数进行分割

```cmd
split -C 100k test.log
```

`-C`参数表示按照所有行数加起来的最大字节数进行分割，同样可以使用`k`或者`m`作为单位，其实效果和上边的`-b`差不多，只是在切割时将尽量维持每行的完整性。

### 按照行数进行分割

```cmd
split -l 1000 test.log
split -1000 test.log
```

`-l`参数表示按照行数进行分割，即一个小文件中最多有多少行，`-l number`可以缩写成`-number`，上边的命令表示按照1000行一个小文件进行分割。

## 注意点

这三种分割的方式不能混合使用，如下：

```cmd
split -l 3000 -C 100k *
```

会报错`split: cannot split in more than one way`。